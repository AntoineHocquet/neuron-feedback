# FitzHugh Feedback Control

[![CI](https://github.com/your-username/fhn-feedback-control/actions/workflows/ci.yml/badge.svg)](https://github.com/your-username/fhn-feedback-control/actions)
[![Python](https://img.shields.io/badge/python-3.9%2B-blue)]()
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

Closed-loop feedback control of the **FitzHughâ€“Nagumo model** with a learned feedback law  
\\( I(t) = f_\theta(v(t)) \\), trained to minimize a quadratic tracking cost

\\[
J = \int_0^T (v(t) - v_{\text{ref}}(t))^2 \, dt .
\\]

This repo demonstrates modern **ML + controls** practice:  
- differentiable ODE integration,  
- neural or polynomial controllers with input saturation,  
- config-first design (CLI overrides),  
- clean MLOps hygiene (tests, CI, reproducibility).

---

## ðŸ“‚ Repository Layout

fhn_feedback_control/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ control_nets.py
â”‚   â”œâ”€â”€ fhn.py
â”‚   â”œâ”€â”€ train.py
â”‚   â””â”€â”€ viz.py
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_integrator.py
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â”œâ”€â”€ Makefile
â”œâ”€â”€ README.md
â”œâ”€â”€ pyproject.toml



---

## âš¡ Quickstart

```bash
# Create environment
python -m venv .venv && source .venv/bin/activate
pip install -e .

# Train with defaults
python -m src.train

All outputs go into:

runs/metrics.csv â†’ training loss log

outputs/*.png â†’ plots (state, control, reference)


## ðŸ”§ Config Overrides

Every hyperparameter can be overridden from the CLI using dotted keys:
# longer horizon, stronger input bound
python -m src.train sim.T=40.0 ctrl.umax=2.5

# polynomial controller instead of MLP
python -m src.train ctrl.kind=poly

# sinusoidal reference with custom amplitude/frequency
python -m src.train ref.kind=sinus ref.amp=1.5 ref.freq=0.25

ðŸ“Š Example Results

After training, youâ€™ll get plots like:

Voltage vs Reference


Recovery Variable


Control Input


## ðŸ§  Features

Differentiable RK4 integrator (PyTorch autograd-friendly)

Multiple feedback laws: MLP or polynomial (with tanh saturation)

Reference generators: constant, step, sinusoidal (or plug in CSV)

Safe training: gradient clipping, bounded inputs

Reproducible: seeded RNG, config-based, CI-tested

## âœ… Tests & CI

make lint     # ruff lint
make test     # pytest

CI runs automatically on push/PR:

Linting (ruff)

Unit test (gradient-flow through rollout)

## ðŸ“Œ Roadmap
Add quadratic control penalty (\( \lambda \int I^2 dt \))
TensorBoard / MLflow logging
Adaptive ODE solvers (torchdiffeq)
Dockerfile for full reproducibility

## ðŸ“œ License

MIT
